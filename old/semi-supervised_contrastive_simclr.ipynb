{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68efe8e-e1ef-4d8f-b449-32f4a2889af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f99d7-86f3-410a-92f6-febd7c1cc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer: https://keras.io/examples/vision/semisupervised_simclr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428b2cc8-a953-49d5-80a1-d73a7ad40ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f79319-73f7-46d3-8ac6-5e86ee95afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset hyperparameters\n",
    "unlabeled_dataset_size = 100000\n",
    "labeled_dataset_size = 5000\n",
    "image_size = 96\n",
    "image_channels = 3\n",
    "\n",
    "# Algorithm hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 525  # Corresponds to 200 steps per epoch\n",
    "width = 128\n",
    "temperature = 0.1\n",
    "# Stronger augmentations for contrastive, weaker ones for supervised training\n",
    "contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n",
    "classification_augmentation = {\"min_area\": 0.75, \"brightness\": 0.3, \"jitter\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0664413-e621-4867-827a-5b37bb8e35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    # Labeled and unlabeled samples are loaded synchronously\n",
    "    # with batch sizes selected accordingly\n",
    "    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n",
    "    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n",
    "    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n",
    "    print(\n",
    "        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n",
    "    )\n",
    "\n",
    "    unlabeled_train_dataset = (\n",
    "        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=True)\n",
    "        .shuffle(buffer_size=10 * unlabeled_batch_size)\n",
    "        .batch(unlabeled_batch_size)\n",
    "    )\n",
    "    labeled_train_dataset = (\n",
    "        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
    "        .shuffle(buffer_size=10 * labeled_batch_size)\n",
    "        .batch(labeled_batch_size)\n",
    "    )\n",
    "    test_dataset = (\n",
    "        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # Labeled and unlabeled datasets are zipped together\n",
    "    train_dataset = tf.data.Dataset.zip(\n",
    "        (unlabeled_train_dataset, labeled_train_dataset)\n",
    "    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, labeled_train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f97207-c1ae-4118-ad62-5cd6681f9cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 500 (unlabeled) + 25 (labeled)\n"
     ]
    }
   ],
   "source": [
    "# Load STL10 dataset\n",
    "train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72eed407-745d-4f8d-8caa-589b5c1f0e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ed69e6-f083-44f8-8291-542b38e63f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distorts the color distibutions of images\n",
    "class RandomColorAffine(layers.Layer):\n",
    "    def __init__(self, brightness=0, jitter=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.brightness = brightness\n",
    "        self.jitter = jitter\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        if training:\n",
    "            batch_size = tf.shape(images)[0]\n",
    "\n",
    "            # Same for all colors\n",
    "            brightness_scales = 1 + tf.random.uniform(\n",
    "                (batch_size, 1, 1, 1), minval=-self.brightness, maxval=self.brightness\n",
    "            )\n",
    "            # Different for all colors\n",
    "            jitter_matrices = tf.random.uniform(\n",
    "                (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter\n",
    "            )\n",
    "\n",
    "            color_transforms = (\n",
    "                tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales\n",
    "                + jitter_matrices\n",
    "            )\n",
    "            images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)\n",
    "        return images\n",
    "\n",
    "\n",
    "# Image augmentation module\n",
    "def get_augmenter(min_area, brightness, jitter):\n",
    "    zoom_factor = 1.0 - tf.sqrt(min_area)\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(image_size, image_size, image_channels)),\n",
    "            preprocessing.Rescaling(1 / 255),\n",
    "            preprocessing.RandomFlip(\"horizontal\"),\n",
    "            preprocessing.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n",
    "            preprocessing.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n",
    "            RandomColorAffine(brightness, jitter),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_augmentations(num_images):\n",
    "    # Sample a batch from a dataset\n",
    "    images = next(iter(train_dataset))[0][0][:num_images]\n",
    "    # Apply augmentations\n",
    "    augmented_images = zip(\n",
    "        images,\n",
    "        get_augmenter(**classification_augmentation)(images),\n",
    "        get_augmenter(**contrastive_augmentation)(images),\n",
    "        get_augmenter(**contrastive_augmentation)(images),\n",
    "    )\n",
    "    row_titles = [\n",
    "        \"Original:\",\n",
    "        \"Weakly augmented:\",\n",
    "        \"Strongly augmented:\",\n",
    "        \"Strongly augmented:\",\n",
    "    ]\n",
    "    plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)\n",
    "    for column, image_row in enumerate(augmented_images):\n",
    "        for row, image in enumerate(image_row):\n",
    "            plt.subplot(4, num_images, row * num_images + column + 1)\n",
    "            plt.imshow(image)\n",
    "            if column == 0:\n",
    "                plt.title(row_titles[row], loc=\"left\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa1e0c3-9947-452a-91cc-e95ce1675452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_augmentations(num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91466f37-bc81-4be5-bebd-276b3cd12367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76024d36-bdd0-484a-971e-b3a4c130a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39424 files belonging to 154 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size = (64, 64)\n",
    "\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "                \"datasets_new/Multiband_Brodatz_Texture\", \n",
    "                image_size = (64, 64), \n",
    "                #subset=\"training\",\n",
    "                #validation_split=0.2,\n",
    "                batch_size = 1,\n",
    "                seed = 1234,\n",
    "                shuffle=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d450801d-4a7d-4994-b2c2-64a2f9e4400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes =  154\n",
      "n_samples =  39424\n",
      "n_batches =  39424\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(dataset.class_names)\n",
    "#n_samples = len(dataset.file_paths)\n",
    "n_samples = len(dataset) * 1\n",
    "n_batches = len(dataset)\n",
    "\n",
    "print(\"n_classes = \", n_classes)\n",
    "print(\"n_samples = \", n_samples)\n",
    "print(\"n_batches = \", n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bad8d0c-435b-480a-a4b2-cb7d539f5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_training_batches =  31539\n",
      "n_testing_batches =  7885\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#    80% (training) + 20% (test)\n",
    "#\n",
    "# we keep 80% of the dataset as training\n",
    "n_training_batches = np.int(0.8 * n_batches)\n",
    "dataset_training = dataset.take(n_training_batches)\n",
    "\n",
    "# we keep 10% of the dataset as testing\n",
    "n_testing_batches = n_batches - n_training_batches\n",
    "dataset_testing = dataset.skip(n_training_batches).take(n_testing_batches)\n",
    "\n",
    "print(\"n_training_batches = \", n_training_batches)\n",
    "print(\"n_testing_batches = \", n_testing_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406fac24-9daa-450d-b50d-7b33bb2ca93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys \n",
    "\n",
    "libpath = \"lib\"\n",
    "sys.path.append(libpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df96c0e-3061-4d98-8a77-66db9d4b1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"split_keras_dataset_to_xy\" in sys.modules:\n",
    "    del sys.modules[\"split_keras_dataset_to_xy\"]\n",
    "\n",
    "from split_keras_dataset_to_xy import split_keras_dataset_to_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac77d7b-c468-4452-b8d0-60d2a6d74dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_shape =  (64, 64, 3)\n",
      "X.shape =  (31539, 64, 64, 3)\n",
      "y.shape =  (31539,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = split_keras_dataset_to_xy(dataset_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb81d5e1-49b1-4adf-a570-fab4ff2eed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_shape =  (64, 64, 3)\n",
      "X.shape =  (7885, 64, 64, 3)\n",
      "y.shape =  (7885,)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = split_keras_dataset_to_xy(dataset_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28fbf270-f5bd-447a-8571-75117fe47047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (31539, 64, 64, 3) - y_train shape: (31539,)\n",
      "x_test shape: (7885, 64, 64, 3) - y_test shape: (7885,)\n"
     ]
    }
   ],
   "source": [
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b202e4e-50e7-46b5-831c-8e646ab991b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "image_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3237526-6b58-4eaf-bfb3-14e6bf8f94ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABWa0lEQVR4nO29Z5hc13EtWrtznOnJEROQcyIYAZIgGESRFJNNyZRli5Js2law5PAs6T5fZ9m6TrLuZ11dU5YsypKVmQNIEAQYABBEIHLGYAaYgBlM6JnOcb8fPTzrrHkEAZnkgHbv9X34UD1VfXr3Puf0qdpVtbbSWouBgcF/fzgu9QAMDAymB+ZmNzAoE5ib3cCgTGBudgODMoG52Q0MygTmZjcwKBO8o5tdKXWrUuqoUuqEUupL79agDAwM3n2o/2yeXSnlFJFjInKziPSKyA4RuV9rfejdG56BgcG7Bdc7eO8VInJCa90lIqKU+pGI3CUi573Zq50e3eoOlF4UQ6TLSIUlJ2SCdHGbHNY5S65U7JgMS8qSRxxO0rkDypKdgh84XV/Lgwz0W6KacJNK5WwvnDheMZ0nu2K2aMmRvI901Ro6ydaRLuOIWvKAC3YFrchO6SxkxWN05W3vK2B+3JrnQ6sA5OYCHz+UwfhTmKvhIo8jKZgQb4p1wRTGUUziswshHoejUANdfIR1bhy/2Ig5Lvr4snUnYFc/xA8vn9tjyboBY8o4G8ku7xiyZK/KkM6pcQxVYF0hjWMmMpDH03xe4hkco1CsmnL8QUsO266PgqNIdhLG3DnrWeVwlT4v3peU9FiGT8Yk3snN3iIiZ2yve0Xkyrd7Q6s7IE+1X196kbqGdF2Fmy15h+N50r1su2jXFvss+YOuANk9rA5b8nf9/GPStAITVWW72XOf/TjZFVf9lSV7n20gness5rAQwfHSR4fJLnUGF8Rdw7NI99FsEp/V82nSHQ88bsl/U4+fuFjBT3aO1ClLdrubSFc7jM+OJ/BD05ALk13OscKSC3+WJJ1rzVFLvnMffgi+neEbdU8RN8jMfXxxX3EgYcmpN/DZ0dWVZOePYf5jL3+fdIFm/PCmvogfgomF1WTX/BrG8fmvZ0k3p6XFkgu/B92Jij8ku5HQ/7Hkme7jpKvMtlqyZ6yHdOPHMHc7TkJ++ngz2b3aNQPvSfwS6cL5f7DkdRkcIxpIkZ1ei7mr/hypxFdfulafvHeznA/vJGZ/q1+P/19MoJR6UCm1Uym1c7SQfYu3GBgYTAfeyZO9V0Rm2F63ikj/VCOt9UMi8pCISEtdi/7+DaWH/4zDJ8juyrFVlvxxRwXp7izCgXBF8OTK+fmJlE/BLfbX50g3MAvHzJ08YslNeXYdlfOLljw+61HShUbhSiZ34avmB4Nk569LW3JrXS8fP46ndNa1j3S7/Rj/ORfeFzrBT/bMYAdk4R9QRx6nRDugG/az+5mf3YX3zPh10lVUw6uIVB+05L4T/LQq+mDnCrKH0RvCnKSqMMdhH/uf9TPhfeT3cEh15iQ+Wx6Dh1HdxtdOfqHXkp3L+NrxHIcuOxvnTNfw3P9H8guW7Cu8QLpF8c2W3JwfI93cGoQhN43DVb/tID/3Yhpe4kZVQ7qnfd+w5Bfd/9uS44UzZOd+ftySWyLsSS34TMnrKrzNEtw7ebLvEJE5SqlOpZRHRH5FRJ54B8czMDB4D/GffrJrrfNKqc+KyHMi4hSR72itD17gbQYGBpcI78SNF631MyLyzLs0FgMDg/cQ7+hm/0WRqyjKmZtLq7S7GzmC2HB61JJXDF1NulUexNjtYcR46g2OlZtGkKq4+iDH86cPId4+1oG0y9nvbCW7mv4HLDk4Je7KJmOWXFmD2DNayyvdq3z4bvM1T/FEF9Y1e4ZeJd3uxG9ZcnUWcV0ysYfsXBrjiEzJOuQiWAUPNCHOdYc5TszX2GLbXbzWOiP8qxjjrP2WPD6xkeycZ3BMp4PTd3IGc+dwL7DkEf8HyCy+FDH16LW38/EfvcySs9nnLLlwaIjsZi88Zsm1t46Trlhch8/adL0l71zH58zhwcp3Uv0K6U4F8Pp4kZ3XvamXLXlGCNdwk+9Gslteibm6O7medLemsb6UlBss+fkCp+hebMZ1tXXRUdKdao6IiEjGvUvOB1Mua2BQJjA3u4FBmWBa3fiCpyiJlpKb6Y6xCx6rgKu+6QkuRKnaP9+SF0QOWHIx30J2C9Q5S75jSsWYow9uZrwKKaNnCm1k98zfI31y+MrfJp3rcrhfVUEUCnp2c/quYEshDcVOkW5fbJEljxT4tzaXQdrMmVtuyUpHyC5UAdfa6eWUVy6I9FLuIFzH4sRcsstnEA7FAo+R7r5efN7mzyy25EzN75KdtwuhkSM7SrqJVhQ8eQ4jzZfXHDYVnaiWdMyPkS5ThVBJd6Neq3CMi53mjtpCmQVTQq9lKNAa3vqkJR9azaGL24vrxZWPk240NMeSKz33ki6d/CNL3uFBSrC67mtkd9SB+bklx0VYC9IIvZrbkfbrrOfb82N3zbbk/us+TLrnYqW07dcLXjkfzJPdwKBMYG52A4MygbnZDQzKBNMbs2cyMtpdit+aXllDusQIYtlIjEsBF2qkT/zjiFHHAkfIrmjrFHvUHSHdYoUYtb33ckv+ZNscsrt/aJsl942fI93jvYh7X00ifXf8LJes7oij1PXoKMd4RzTWGdqyL5MuaytZcDhRIqudq8kuEEMT0Xgvr30UcyjVdWmMv6g4ZRRVGyzZn+TS4hmP4/Xult0Y3y8lyC7caEuNnb6edBKylfumNlmyinFFtd+H2Dk1l0tR+2aiIcWbQpy7VB8gu8uH7rBkzwg3WKWWI2W3M4ly5H1ubjKJVGAcNc4B0oUd0GWzXyDdgAOxc+0I0nkTc06TnfpNrEfoMU4PTvwFrs3Maax1pHw8V8OvYj2pIbyddJ8c/KiIiHw/xmtVdpgnu4FBmcDc7AYGZYJpdePzuk6imVI6SyU5fRIYhsvZHv466ZpakIorjtsa7Twrye6kC33OJ7ytpBtIww2sL6JfJ9jPPdRXzkA6qflomnRfbGy35N9sRkfS0SWXk91Tcz5jyS88xe7WRO57GGOWXevQOFJN1XF8l0CBjxGzkVm4ClyRFlRwrYcV3OeA2k92ASdc5lmV3Il2shlhyWgR4YSqYf6AUDOOH5yxm3TxxMcsOZrfgvHGI3wMjedNzM8VY74xkDo4h/ZgHM2cXnOuQ7pKD80nXca9zJLfaIP77znErrSag5TVkJ/JSFw2cglXmt1/CT5sidmuP7Pkiu18a+klcPH9Ea7u/E41ws/dtsrPe6u5b//MAM6Zz8Hp3o72Ungcm0KuYYd5shsYlAnMzW5gUCaYVje+6HZJorlU1aVWPke6Qg3c4jbF1VjBM3CrnBNwqRJRbu44bHMzvU4mWmhMo8oqk0TjxLlUB9k9HkG1V2oKx13LSTRgXLsaGYMrGtlFvqLqIUu+7SxXrm0ZRAPKgdRM0p0cwOcdz8PNrChwRVelhi7kOkw6lUYTUZO6xZJT6g2yK1TAdbzuWnZNzyyIWnK6GivzvlE+LzXNqH7z/YjDMqlE5WC2BhmI5DBTPqk0wgT3OLvPesx23lMY05KuefxRbTYiDsfjpBt6HRWAJy/DHGgnhwwuW/Ge082NNpEiXOtzDm488hQxxoHLbBWFmzgrsPIphGz9K7iC7j/W4ppInoDrfkMXVxRWNqMyMzPGuk3BUiZnIs/hiR3myW5gUCYwN7uBQZnA3OwGBmWCaY3Zne68hOpLcV9VK/OpJ6qRSpgTZ7rhXABxTDKP36eT/Zya8OZQtTVewVVh3Ym7Lbm9gNi+0MXxcDiPY3Yv5mqkwaM45t5xxJ6fDV9BdguLUUteewPzk688Al2eeRMlkUYceWQY87Mxy2mW9Q7EuYNT1hWaXaiMqywg7aRynKLzTjxlyYs3MCnmi89jjJkQ1hwqXZwy8t+C1KS6aRvpHEexnlLlwTpCtCFKdgHbfgHpgQjpXB/G/M94FZO1vJuJG5wDGIdzHm9bcPLgZy3Zuw7vSw5uIbvki6iuc3Tw+sN4GmnhYDvH2y1BpHjjAawnFVdzzB7+JtJ+G5u4QrS/DZWJq1bju/Rt5cpMNYrUrz/Ft+78xtLahM99fuIo82Q3MCgTmJvdwKBMML1uvMsrkcaOyVfsbt10Di5K8wTzgwVySGmMCtzifS5uVDnSZ9sGyMNNFc0+7ORxLrvckiurmA+sX6MJoljF6TvXUhBghLfCzXbcyU0Pp2oRhoRtO9iIiBzxLbTkvginSZZm0dhz9RDCicsUu3O/7UP6540shyubxjHmV3N2XjjerOeBLJosQhne5SRShKvqiyG0CP8Ff1amYHPrV7Br6h7GmIu12AHFmefGnVAa7m0uwRWLwRloUrrxWujaXGwnqfssMT+2lFTdVfgu3k3YBcY3wg1KyX6k5Yo+rgZ0oShRAtcyP2J6ObjmgjlUIlZcxee28+g/WvK3XuDUXvY2cBF2FOCGexdzpaDrFBqixg4sIV1+Qem6zefP//w2T3YDgzKBudkNDMoE5mY3MCgTTGvM7pC0hKSU/hiM8L5ebXF0doU8HCune1EamLNt4ztQxTHNXCdiOT1lu+XRwE5LdquPWnJfo4fs3K+/ZMmqgWPxqG073dY+xGdP7udU4XduQ6rsuhzv19UYATf8ifm8W9aLdfjslnaUm66M8fGXnkTn3y29HM+vc2BcQy4QFB4uTil11dhbL5nlrr1iBmP229KIuo/TiLG/ud+S3V+bstPsbMSsmUV7LDlwgvnUZfQ2SwyFOJYNjmCtZnYQaUQ9l9cYHCPYt238+FnSpVOwVXmkA2vC3BXp0UhNDtV/iHS6CXM80cSEEk7/31vyWB6pt+op5BgTy0FsOvQYr0n5t6B7M1vxT5Yc9fPtWdH2miVXHmPizlxXqRxXZ6JyPlzwya6U+o5SakgpdcD2t2ql1Aal1PHJ/6ve7hgGBgaXHhfjxn9XRG6d8rcvichGrfUcEdk4+drAwOB9jAu68Vrrl5VSHVP+fJeIrJ2UHxaRzSLyRbkgkiKFUoqj2n0/aeY0wMVyDL5GuvF6pJq6euHa5Wax6xiuQeN+9PRlpFPHUCEVtvGiF1fdQ3Z9BZub2f8Kj+MyVF01rUan1Q8HOGQYLN5tycdCzIHvd8GdnhjhNFGqH+mZwblIww30Mhf4K3l0m7Xb3FsRkYW9cDMXa6RxbtHcbZa3pdTG3UXSuWynO5C/zpJHZzFPv+uDJ3G8FVy5FfK9bsneYTxTvI3c2eZz4JgBN5ORhIs3WXJdN0gpCnF2YfMJhFsH00zEMZLAtdNQQIfgxJm9ZJfRqADUfdxVV0hhS+t0w1+SLt2E8xRx4jxVpevIzrkK5yx5P6flZv07xu9oQWin/Z8kO9dchDl5L1cRSnLyVi4yH74d/9kFugatSwnpyf/rL2BvYGBwifGer8YrpR5USu1USu3MjCQv/AYDA4P3BP/Z1fhBpVST1npAKdUkIkPnM9RaPyQiD4mIVC6br6NScs0qFTfC+AtoHNAzuNoro7DKfmQX3K/cy7wy2nUT3peti5CuqQjXr2cEcvwwD711ESrchg4vJ533Q1ixjeXgjh/NRclONmNVvXjbz0mVFHzvGg83Y4zYKu8a+kC64CjyyvF4GtTJXWeYVrk78UNLPmKrrmtws/v8wVqsWoez3FC0pA3zPS8N93Mo/HGyO/q5f7XkXYrpl4dsjTz+CFap69z8WcXtyDokRnmV/QMj2K7JE0VDS1xzliSjuy15T4qr/KI2Lr+4bUswT4hDwGIGDyKVYYryzOu2cSR5u7BKDSKU4Docf+aUO+tfnTh+6lre8bb9ecx3cCfmrTB/B9kNhlCJ2DzrJOnG9pZW/4uFd59K+gkRefPMf1xEHn8bWwMDg/cBLib19kMR2SYi85RSvUqpT4nIV0XkZqXUcRG5efK1gYHB+xgXsxp//3lUN57n7wYGBu9DTO/2TzohE8VSpdy1Ho5HKmwkBgnheD7ZErHkA+1Y+I//jEkj9AhSdnX3PEs61xxbp9QAuuUmdnPcP7AEcWNh9Cekq3kFcXTvsl+25GzqX8nO04u0SGWaq6XOVXVaci7E8VWggGq+cxPdOEZmF9m1utDxpFu5iywxiuq3Q4J4e0kLp7WKQVvl4MYI6Vps2yg7bwQn+9WbOI3Y+OQCS953M6cfu45h/eHAEaREtwW5Y21nDWLqiJMXcPUo1gH2u75jyf+S+BTZLdQgc+x1V5Au5kIlW6ECOlXB/OoTblRpev08RrerG3L6WtLlfo4xDtrISh2Leavu5gnMnb+Rv2fgDjjYIyM4n+4od4b6hu605HP1XPVYHSmlPp0uXg+ww9TGGxiUCczNbmBQJphWN96dy0j9QMnVWVk/hRjCB7KJxBBXjJ30wiXsXg23qfoQc2d7N8M9SoemuIT3gHeu6W587eQY87rHB+GKjc9iXvrLnkKDS/dKEA4UqpjQoEEhndfQwzzjoxU2LnQ/N7i4Be55/bzNlpxSXOHm7P6RJWvNFYDBZ+G2zsoghJizjdNVMhdu/ZiT3daeUyDc2JGDu5u7/GGyW/O3KJpccIxTb7eHkJL6sBcp0XMTXWT3egIcgM84O0n3vU5ws42fwHOpIFw12D8OkovKfuaIq00tt+SqiY2WfKKer4/sDFsYUuT58PUiTamLnMIcO435v+LRn1qyYyZfw89m8F3Cbg55QjZ6QM84vmf+u3zt5DNI9zqLt5FuuLMUluU9TLxhh3myGxiUCczNbmBQJjA3u4FBmWBaY3Y94pTi90pxaraT49XNNyEecQc5Rt03hjhUN6JjaPweTtGFepfjxYtR0tnTKcPXIqZ2z2PivswWTIm6gfnUfYvweftfR2xUXP0ZsmsbQ+yW9/EYXWHEqM4AE2eEEogNE0UQVLh7OT04chodZVWzOH6tqPqEJbeeQWGjyz+lPPQU0msDVXwZbLd1xJ3aibWOiuuY4HNrPmLJr7zKPOl1KxGzz2kCmeOV/jlkt9q2XfENIxxvd3lw/JfCTZa8xTNIdmcKn7fks2Huvot5sFYx7sC6UF2WrzHVh/nJO3jbZ23bM88vTN3gnrfOkjs7llvytgKXQscqsL31ZekpZKhBPHN7l6zBsRdfT3bhg0jpRlv+D+nyM0rzX1Tnv6XNk93AoExgbnYDgzLBtLrx8ZRDthwouYVHDnOFUXIC1U2eezjltboCqaDZZ5EyGZvNqYnCvXCPqv9hMenOPgJ3LnUyaskNc9lVdznhSgdGEqQLzoT7OPwjpE9UnN3b8DVrLbkyxCmpsTzcx6yXq+sSLoQXgRRc98Y0u/tqHF1wys8VXYFlcJOXjyO9VnliJ9ml0ghDEuNcdZXy4rX3ELZC0nMeILvBm6ALbeOwrE//gSWf8nVb8t6xCbIL1SJ9+tEQ8xIuciBEWVQFkovfGWE+vZ4mVD1u9XIV949TIJE45LkdiuQmsgsFbN9FOCTpdCFsykWY0MTrBdlE80kQOr3809VkF/hlVL81uLnq8Zzg+nYuQ3qzdu0PyC5xAt9bv8Gp1OzKkk67zJbNBgZlD3OzGxiUCabVjZ+b98r3h2eKiMgzcph0GzbCLdlbw6utT30IzS/VHtBHzzvBpA5NS7CimryPq7HUpgheTGBHUMcLvD1TRw2q5FJn2X3ubcQ40rYQIrCdG0RS16Cq6ljwmDBAbOEZ4+kvBl625AovMhCBecz95jjzEbwn/SLp3JVY+Q60guNOpe/mz+r5sCXnxzjUUDYaM4fGOLyJn5KdZzaajaoq9pFOHwTJQ6IdDUuJOm7gcIdByODNMzW4L4nPy1RCHvIwC5ojidDgDuGV9A9FEabtcoM++4W63yK7g5W4Hs+c+yvSRccQHi48zpmR2Qlccx0FEEq4ernRxrWww5JrV3EjaVsSYeBJH5q5HNcGyC66B5mA4CtctZnomgwJM+8+eYWBgcF/MZib3cCgTGBudgODMsG0xuy+YljmTJRSI59xdpDuEymk4ga/xdVN6w9jmOsfQBy9aeYCsvMnQb7YZtsGV0SkIoKti4ojSM/ETvD2SR8/gBgv6uVxvBYEF33+BsR4da9xGqfwGtIz+bubSKe7ELM6ixyLxzt+1ZJdlYgNx2YxR/iM3f8OOwen/ZpDIINwLQXhgzPI6xvpGNY09iQ/RrpUEt/b4fmGJfteYyLGQA1i7JHGKVsxHwLxQpUfnWITzdvJzp9ZhDEWfoV0305ttuRZRayl7E3ynB6zbYvdOoUY4vK6qCXPz0K+Kj9lvWQMXPE7Tp4g3atjSAmeLHKKcZYL10/ERs7p7ubuuAWbsA7lb/wR6c60fw669FpLHgsxaUntNfie/tNcydf307OT34M/1w7zZDcwKBOYm93AoEwwvRx0zqjEqx8VEZHKADdEeDOoWnKHmUfs03uRC/qNryJ11fXpzWT32AK4vi/EmNhidy1IEzxZNCnMXDiT7B7JIE20fDBCuqEuVDTpW5GScjdyk0nyIJpd5J8eIV1qFlJSnqt5+gMKLn7CgfRdrokrqVquRsWY7yinB2fl0Uzi1HDpk8ERshtsQqXgweHnefyur1uySkcwpgJzvg/0RS053xQhnbMFaaPxPXB1A+2cXmsr4LOj6pdI9xU3jlk1AT74pkYOXVJjyy15UHMq9WgU8xMeRzqsscChV1se1YzXO5iv7xoHwpeUZ4qbnEElZd6J7+kv8C6u+9+A290f6SbdrM9iH4N5fqQHg4p5Gnvn477I1XIFarWvdG5cU4hO7DBPdgODMoG52Q0MygTmZjcwKBNMa8yer8jJuXWlFMRJN6e8Ko+tt+QNY0z40BhE/NqZRXqtiRvW5HPL0aX2YIFjw+5OxEw/HUFn0K4Ep5MOXIatjT/Qw6QUzSOIw6pewfjDMSbPTPUivtQe7uSqcGKMxa3ctVfs3W/JoWs68PdKJqgYbEVc6o3y96yqxPpGXhCXj59mAsS9c20dd0f5XDhdIGb0FRDnxhR3rHlGsFZR4+U1gVgQ4yju77Bk/2pO0bXNxXy8kGbiiWQOMXwk/5glJ3L8jMqOd1vyaD9vWVxw4Htnkih7PePmFN3rTqx1PFpcQ7qrbFzst4c4Zp9VaYudJxC/r1E8jlgW89gdi5NuyznM97YKpHdbR3jb8eZRrFd1zuF1rXy2tCalJrhM146L2f5phlJqk1LqsFLqoFLq85N/r1ZKbVBKHZ/8v+pCxzIwMLh0uBg3Pi8if6C1XiAiV4nIZ5RSC0XkSyKyUWs9R0Q2Tr42MDB4n+Ji9nobEJGBSTmmlDosIi0icpeIrJ00e1hENovIF9/iEBZO55X8brT0kSeC7G78+Uy4WKeG6kh3pgcu4q4JVFw1foO3x2neiI6fBb/FrlK7B+7iHy+Hu5U7yW7ZUArjcAlvi5sfmG3JlTuQvhsMM8mFewZSQ54BJuIY3xa1ZL+Hx1gZRrrKcxbhSqzqBrLLNeP489NcXReOgWtPtaIqbOjAWbI7EUSXYXoeE32k9+N7h5cjDRU8ypzvzQWMv6/A7nlWwS0OB+DS+vkQUmnrVHxFc/edM4DusPpz6HTLFXhbrqKNBCTdzMfw2DrpxmO2NFyKx5sPgczDHf886fYU4D7XC5NGzLRVWRZacU3PnM+h0d92oFrSu4a3fT4YQrpt/ShCwhc0305bFebjjdlPkC6SL6Xv4jveJQ46pVSHiKwQke0i0jD5Q/DmD0L927zVwMDgEuOib3alVEhEfi4iX9BaT1zI3va+B5VSO5VSO3PpxIXfYGBg8J7gom52pZRbSjf6D7TWb5aEDSqlmib1TSIy9Fbv1Vo/pLVepbVe5fYF38rEwMBgGnDBmF0ppUTk2yJyWGv9jzbVEyLycRH56uT/j7/F2wmpcFH2XF9KeYRnRkmXcyHeTj3HXU2VR7thlwSH974ZzDN++gW8b/Me/mGp+mPEoSvmYIvfpXXMBtK2Ge+L975BOqnEZ99Zi86ofQUORPe/jg6zwVFeE/Bk0W1WHeE9vwqHopbsDIK1xtH+YbKrqwTJ5Ir6g6TLCdJ+w6M43uhiZu4Z2oqYcmIRl5/6t8NWjWBPseSdC8kuth4dcS4HO3t1IXy3jB9xbtPmKNk5qq+y5D1NHyBdSGOMuglpqKyb1xhE4zwVPdxVl5hAKWq+Hwkj1xBv910Rw/4BrgCntSK+r1jymhDH7P4Q5r9/Hj6r5zZmW+qt77Dkdjcz1bSnwDb0+2F0Kn4xx6Wvu4tYr3omyOs4zw/9joiIaOc6OR8uJs++WkR+TUT2K6X2TP7tf0jpJv+JUupTInJaRO67iGMZGBhcIlzMavyrIqLOo77xPH83MDB4n2F6t38adUvuRyV+7tlL2FXKXWerHruM0ydDXbatoY6gMywVZdI9xxVwrfUrXNE1/jdwrV+5DR1xXS5OkdxXBH+4p4pd/CUJpNHa879nyffnXyK7lHOvJR9xRUj3rKAzb1sxSrpzcSyhDE58ypIbmSZdIhVwOT3+fyBdj/c5S94y9qglj/p5eaYvjfSPcz+7hJl2hB7DJ2w85DcuJbvT9f9myfVxDmXOLEGVX7gA17ctxdWRm16whVv3vEy6Bvf3LbnKg+eNdlxHdo4Ytt3OTHDVY8z1aUtWGnOfdfH2SSkFspPWOnbVr8wjJGlbxNuFOVoROo4FQUJxzM/ptWwKhCbbAg+Rbl4BKeQajYrLTi/XqS0MoFrviihvL/WFVaV74e4gby1FYz2vxsDA4L8VzM1uYFAmmF43Pq2leHCyqeAI+6ZHDsCtL9zF3N+Oq9Ew0psHB9jEq0waMceH6qPiQiYPGDoKF652J6qqGhYxp1ggBjILleEKt1gW7voPinDxVye5nqgl/fuWfFWIx3GNYOU7r5jje0cUrvCzCXT5vDS2nux6I6hXOFjLGU93Cqvn6WZUMM8Z4bmqmw03MznwOumS88Bxlzv9PUv2b2C3svhBVBTmfhbhcXThHLqvxs6yjU7ehurnL33VkrNvcFageh6OOeTAufA4uXos7UbIphQ3Hmn9m5bsCMINdsaZo9DTDu7/YQ9nScLjOL/ZUzzGdB2uq94hHDPl+iDZNbVHLHnM3026bh+adYZdCGt6hK+/Vtv10qB5P4LGdInoouB8ywy4iJgnu4FB2cDc7AYGZQJzsxsYlAmmNWZXBb84J0rpm9mtXB00ega/O8lHOT3jn4uOpMKNiC91jlNjo68iFnfOZZ70gnOFJfdsRv3PvSPcNSbLkDaLel4jVX8lYsPjGp15h2O8z1mFGynBTh/HkCuHkDZaEeVOtFuziPnuOA1e+mg3223txNrEE0UmYXjFu8GSMyGkZ96I/TrZBeah+rDpzL+QLnwa6ch8pY1Q0VbJKCLin4uqxMJ9TMQRfHozPqsFqayBug6y22fTFV7kLayH59mq8hpAIJELcnWkR0ctOevglKtL4bupONYOwvOY9KPQh3EEjjHRx8rDIPj0jvFnj55CPL/nE7imk+eY0HJoNlKC3gwTWg4EkN6cp3H8dI6rAc8VcN5P2raYFhGpqCntixBznZHzwTzZDQzKBOZmNzAoE0yrGy86I45CqcqtMDiXVOG1qETKneBtiLPjcM0iMaSQwss4FBg7DdfUs5dTJKMz4f43aXDaNR3nraNTVeBBG4pwCnBXEFtIpYtw3R0Bdj+jOYzjQJobYQ4Wf2jJjwV42+C5i35uyfcuxTHbFrBrelcFUm+3ObliKlYB8orNo3DBf+b4Ltm9lvlLjNHNIY/HARe31RaFBBOcLvWfQGrIsYrTcvkmhFhNTz1myb33/iHZFa9AU0toCue7N4X0YNFGPuIaZfLBYjvc/Zp2fn6N2hqRPFvh+mZWM6lIVQ/eNyedI52jiLAyU8Xn+mgchBUntkUsObWaK8wDPaig61x8D+kas+gvm/Chmi7giJBdfx73TNHJx59bU7oe3U6z/ZOBQdnD3OwGBmUCc7MbGJQJpjVmdzpyUukvNfXno/w7406AkC98NceQahdiJrUFaah4jMtNQ4tR+hrt6idd5DSOuWg24qeKvVM6vvYg3XF8JZNjDLtQ+qr9iN1cTk4VBlNYVxifkgoK2OP7Ot4aeN+NIKXI1IJw8joVIbvLJzCu2gpet3D7MK932piBbgodJ7tU8GFL3hzira8fqXrBkrfmcYziDC4LrtiPGLv+Ie5inPtr+N5L05ifx3dw3K9v/KQlN9ZyB1/Va1hP8Z8DWWR8PZezZn4X860LHLP6bPuvxfZhX7nGJJM5umr/xpLnV3I8HGlE2i+T5vHvbsG4+oaxhhHs4mundgHGfDbJW3WH3Ijhi4J0aXEKO2fItuV0QnhNKqBK39uhOKVoh3myGxiUCczNbmBQJphWN97rEOkMlFyic94o6dzbuy15XiV3ig12wDUrulGpVdzM3PNV9yBl4l7FrpJrE6qUVsWQ5nO0RsguexIube8+dsVyi+Gq6npU3ikvb/80ZHPxvWFO4xTa8HnFJPPeVxy625IzBVTvVS3i06TdSAHuSDBjb20ELu0eWxquMcFjrIvhfTe0PUC6Dw4jrTg+C27rC8OcGnv8HOZ//1mu3KrogdvaeBdCgRsf4q2Gj7x8pyVHFv4q6XIHQWtYeBFVbMrJ10fhT+GqJz7KLn7FGnDXOaqRSnV2rSa71AJwvy1pfZp0ohFSDYzxdmHHHbbcpC2q1GPM0lYct6UY5cek87nAh5fJIyU65uFQNB/CHOcKXPnpKEx+N/VHcj6YJ7uBQZnA3OwGBmWCaXXjHQW3+GMl91rX8Sp1rohV9hPPMsdYcg3IG9TcdktuGmQ3J/gSXMTx63jluGoGvuqcE1i1L7TyiueBRmyZ9OpZbjJJD4K7LlsBn83XxEQCxarlllzRxyu7jRsRCmRyTM3c146KMUfn1yzZtZdX0h86CK62bzWxO/f7KzDmwx1YRZbWFWRXNwE645b6COnaPdj+qM2W1binjam177+8w5JHtvD5fG0jyCDaZsF9DtzLDTOBZ75tyV2vM5GIJ/U7llxp2xXVlefGI6dGQ45v9+WkS9cgDMnO/xHkZzeT3XUHrrfkUOWtfIwmUHmf0mnSjdoq2cI+ZDEqfByu+PNojEmGuLHJ4wcZR6UXNN6xHFf5DaZut+SZ7k+QrpB7TEREtDYVdAYGZQ9zsxsYlAnMzW5gUCaY1pg9IS7ZWagWEZGKPMcWdSvmWLLvIMd13m13WHLDreh4Gp3L8Xb8MFJqvq1MMjDX1uvvHIjgPb1zyO7IzBmWfDZzC+m83bssuTALsbL7JMeQegBxsysxm3QOH7bnDbbw1k2FUfCkB4exJpBJ8RbW3/MidhubcQfpnvFjfq6qQBwdreHuuEILOux2RzkWf201Kt58ZzDeVVPmakEn4v4mP6c674ijEiz53NctOSOcirynHxVoT035nqc1OgSTRcyjcnyE7Hy5Byy5dcfPSZedBfLMggefrZq7ya5jG9Zdsvf9X9IlXFjHeSnJ3Pnjts+Wmv+wxFwhSnYpwfhrj/P3dMxHJWUyCRKNhiFeH3D5sL6xIMTbS+0Klq65pHBFJX3OeTWTUEr5lFKvK6X2KqUOKqX+fPLv1UqpDUqp45P/V13oWAYGBpcOF+PGZ0RkndZ6mYgsF5FblVJXiciXRGSj1nqOiGycfG1gYPA+xcXs9aZFLAJr9+Q/LSJ3icjayb8/LCKbReSL8jbwOEVaKku/L8ed15KuPw4CgtqbHiNdZA+IEOKHUeFWWXcb2SVt6TDnGd7Wqd3WjOHoQDXW3pO7ye5UD5olIhXtpJMJVG65XkBlVlULu9LijEJuYB7vuAcVWHX5n5LOpa/E2zSq0/Za+2lODiOBtJazl3ftrBjB/IxO/JUlD9dw001lKyrqqqLc3JG3eYK5JXjfpse58ejAchg2rf0r0n30KEKDyAjc0YoJbhr69QTO+6cUP3tOK1QDvqgQnjyaZ868Uxrjj8o46QLPIP1YuRihRijL37k+h0q4nn0e0m1dDRc/N8ZbQ3kTCCXTDnDF10eeJ7v6WozLkZ5JuvR62/zYbsnRGibKaPciZdfewAQvjzf9RulYRd4fwI6L3Z/dObmD65CIbNBabxeRBq31gIjI5P/1b3MIAwODS4yLutm11gWt9XIRaRWRK5RSiy/wFgtKqQeVUjuVUjvzhYkLv8HAwOA9wS+UetNaR6Xkrt8qIoNKqSYRkcn/33LfGa31Q1rrVVrrVS5nxVuZGBgYTAMuGLMrpepEJKe1jiql/CJyk4j8LxF5QkQ+LiJfnfz/8fMfpYTZFW554tZSeuzYq1xO+FwMHVVPKe5qOr4MsYrzNaSQQie6yS5Sh3WAuc3c1eQcQgz5tC322eznbq1MDONKDDGxYThgIydIg1jBHecYL9WC1JgOc3qwsRLxXyHPpbThJMpxZ49hDja7omSXaMI4IvM51eRb8wGbHbYXznq6ye64E+lBW4VwydaJzrGqIuLJpgwnXCJx2IXUZtI5wijP9dous7xw3L+vgGNcmZpBuoVpXBOX5xHzfk7zlt6vC1JeL6p7SfekwgPmzEGsFwxM4Z7/Uy/Scrnj/D2rl+H8VrXxuda2tQR34JctOSu8ZhTvwh53usDdlL4cjpnwY37S/UxE4SxgjDWzeA487lIHpcpzus6Oi8mzN4nIw0opp5Q8gZ9orZ9SSm0TkZ8opT4lIqdF5L63O4iBgcGlxcWsxu8TkRVv8fcREbnxvRiUgYHBu4/praCrysv2e0ouTEtvD+l+O43Fu9/Zy9Veh1ch9bZ+NVyZp7lwTU6nwfne5+IqpQMVcJFVHGkXbz27StUarmT1BPN8nUtjWyCvH+MfHmK3r9W/2ZKLtZeRLu3G9kzaMYW3zY056GyAG/8ftq2LRUT0Coy/fRHr8t6NlpwcfgCK+C6yq6/CEksiyOmwQqfte+eRkkpO4UIfP44xzs1sIV1o9CYcP4EU0kEnp8aeb4S7+8Y57pxrtrm7TbZuszlFXmqa5UBX4GVOdrP/cByh3SuP4vpY/zHe4un5MMKJc7KfdKNZ6CoX8vEbx/E6NBG15Ink9WTn0rjO6vwbSBe3Ebm4bFyMTV1MFjLTj7k66imQ7rX60jESOf67HaY23sCgTGBudgODMsG0uvEjIZ/84LoSkYEvwquGnT+Gm335MFcBLbLRI3+hE9xpn1e84rmzH27xhhyvsq8fR8XY6DmszBdX8TjOvYGqsBo367xxuLTFDMKJiGLXqdANOupiZSfpVMvdluwJ7CVdohNucaEZjUL763n11lGPlfSqF7kBpXAFXPCaV+B2Z9yryC62CHTazgW8Qp7PIizJ5COWHPdydrW2Gq7pnH4mWpA8XN94Eo0wR/JfI7NCDq5qdIp7nnJh/l8uwF0OOvn6aFQgx5hv29FVRKTV9zlLnpkFWcif73iK7P7k75CheaWbQ8D1tqzAhiNNpDs0gcyLv4hGoZY8r/bnKj9kyYUKPn4uhZX6ymi3JUdVnOzC2Y9Z8vZjXIU3OEmikUtySGaHebIbGJQJzM1uYFAmMDe7gUGZYHoJJyUnvmIp/sks5Y6yfU6QQbz+OseGDWdBZrEqjX6b2lkxslvoQzrlasXdSZ8Zgu7A7Kst+WnnAbLb1AzSw556TgUpG09gZW/UkmdkOIXmtsVaM1xT4v4bUNk33sO89yvWY1vpbhdi7/FOjoeDH8C4wj2cDlNeEC6qoyiN82xdSXahWxHnJlJPkq6iATFl3JaSqvFPGW8ABJm1CS7FcDoQOw67sdYxlv0q2YneA53mTq5GjXn12ujg48Ix7xuuX7LkQx6uevSNYYvo6/JYP7k9yBz1Lj/m4zovc/FfH0XM/qcpJo3YYtvG+hnHQ5b8aoSv4a4g1ngC+WWkq59A1WNTHOfsagef2wonznv3CHPb5/btERERneJ0sR3myW5gUCYwN7uBQZlgWt14OZcTx7dKPOT55bx7amHOcktOreXqppH9qKjbeAauZNzLnGh3VSENdU/llN1TbS7R6jTc+IXNHE78P6luS96reHqevwIu+Qv7cfwDO5n3yx9F9VRqP7tsS5/E+Gc2cDps1gkQTzyUBc+4jnIasb0JYw42MHGGow8VaudCSO35B9gFj+6BC65if0e6+K8j7ecSpCmdYU4FzerFeKsmniFdvvhlS+6rQLoq72gju0AULnmFm9srXHnsvBvI2XZjDfH2SbnMDyzZneS5ShUQe4Uq4VoPXPYK2fXaIghPgec0r5E2ayhw6vAGJ+bqhlqkRDMe5trbVAAJ4tNJDjG35RBKjlTimviY706ya5ZnLbkyOqViMVa6NuMFwxtvYFD2MDe7gUGZwNzsBgZlgmmN2Z0jPgk9XEpxFE5ynHvqA4hVims4TZRzgwAirtDhFBmfUnKbRwnhFs3HmLEW8drRYyhnfa0wi+xun48uqZX7+Pg3r0Mclrkeab+XtvKecE8+i7j5lZ71pDu2F1P+0b/kNJHbFm+/vsFGvnHug2RX24/0YKaS01VaPWrJ6XZ8VjLP3WbqMGJZXyt3/gUTa3CMmmtg5/oQ2UUS4GSXGHfVTXSjvHXXcczjYGEP2bX5bd2IyRdIl9aI4QdkkSXXxJ4lu/ociBwmAvNJ55yJMulQCmtBX32Z02uHbsYxPpjmNYGjCue3Mng36ZbVPWLJS7xIMTYXee1gbQzp49uF02PpCNY0drnRcXdZnLspE2NYCxqYUkpbWVda10q5uJvUDvNkNzAoE5ib3cCgTDCtbnzBNyGJ+SVXzXO6gXSd/Uiv9Q9Wky4ZRgdR4Sq42fkznKKTSnCcf693E6mWRuHGj8yHu7X/1DayO2Gr1GpMc2pv3RG4/GsWIf11y0LmmftA42ZLjn/XTbottqq/urnMwzd6I1xt/16kzbzRq8iuv+/TluyIHCZdoBrHr4xhTvtSPKf+friVNZezS1s8iXFUh5HiqZYFZBdpxXbO8RATLezPgP/umAfVdOPnOGQ42QP3P5Di7rtiGrz6YeJ0Y37B7uJLlpyx7T8gItIZRsqxy4fQZesZDtHCW+Cqn1wRIF2hEvOYct5MuvUpuPE78nCtw6ElZLcyjE69OeN8/Hl5hKbXOjos2e/kc5usBLHKZR4+Z2erSvM/+hyn/OwwT3YDgzKBudkNDMoE01tBl8lL8USpcSA2hW+suBsuXPYv2UVxr4MLGvwtMFZ3VF1Hdt9Ow4Xd4uXtjsbGsZobHIL7KUFuNpgYtO0gWzxGup8/g+P/tB5Td7mLG1XmX44GjpWb+kl3z7DNNs3Ve/3XwBVeMo5toiYe5ZXuUxm4+D1jEdJVheHGhYJwHSMRdp8rbbup5gfY9Rvox4ru4k6clxsi/0p2Hu+tGGM97wR75iYQbiSHEDZ5DrCbHReEaDrBK8zzs5jH0xmEQwOeDrLzZ0H64R/kkGrBIN7XNfMQxtTMIUPjMcxVsoXH6G8Br9/ZwD2kcyU+a8lO3/cseTTFdOgv2Lj8dnumkFIUkJm6bRzX1eopDTkVLlTHXTn+BdKtGfhrERF5MM1hox3myW5gUCYwN7uBQZnA3OwGBmWCaY3Zi8ojCU+pokl1cKwsh0EK4B1i3njvD0DoGN2AmNrxDe4eOnwl4qJCH+89WbQxTwwPogKtMsHxaiCD9QFHC1cwje1BSiq/F5VOW9dytd6hZvCkv3Y7x4bX/QC664e48k7NBrnCsg9gHL8c4LjfPfqYJb/q5u2O1reio+/AKcSvjjbuhqppQrdZ0zhvX1xj2wK5tQ+xflM1p+/O1oCw4mCWO9a6cz+05JBt/cTXynOaS+A8jcS5+uuQA/OTPYlz7YheTnbB4B5LTrn5e9ZlkfZ7vguXu17O5z3XiesqfYw3JC4sxZzWuJjgMz2KbbZHo6g2dM/ZTHYOl63qr5/j+b5hpBUr+nF9pFO8HXc+gqpE75SKyMLsPxMREe05JOfDRT/ZJ7dtfkMp9dTk62ql1Aal1PHJ/6sudAwDA4NLh1/Ejf+8iNiz/F8SkY1a6zkisnHytYGBwfsUF+XGK6VaReR2EfmKiPz+5J/vEpG1k/LDUtrK+Ytvd5yCzykTc0uuYIWLU2PJRsiBKA8rpvCbFHBGLDl341myGwmhoUA1sOsbH0TYoI4izaIONJJdeg/SLs5K5mt3XmOr+tuGz86zpy6FeVdYcm8tc5zHr0I6TznZjXe4Ue1VY5sC50Ju7uhIoQJrXpQbeX7Tj3Re72ykjJ4+FiW7p7MIlfYN1JLOnYfthI1Kre/UUrI7HcYgVZKbZFJFuNq5yBOWHHQcJbuKLNJLnjgTW1QMw6WNe0F2knay3bjvAUuuqeWTkelB09OJDI7nOcxccq4WjKN+gskrUhtsz8Q7/i/psiGcs7oWzH2mkUO7iXN/bcnOGDeB1ebAG99QCRe/GGJXfTyI0GjPEKcHn3klKiIiZ+Pdcj5c7JP9n0Tkj0TEPsoGrUt7507+X/8W7zMwMHif4II3u1LqDhEZ0lrvupDted7/oFJqp1JqZy4du/AbDAwM3hNcjBu/WkTuVErdJiI+EalQSn1fRAaVUk1a6wGlVJOIDL3Vm7XWD4nIQyIi4dpO/VY2BgYG7z0uZn/2L4vIl0VElFJrReQPtdYfU0r9nYh8XES+Ovn/4+c7xptweIribys93ce8UzqcgohDM+s4Lafn2LYydsAZGZuyF1tOEAO71F2sa0SJqSMMgr+xMKdSamzpmfiOKdvfLoNn4lmOGM+xh7vv4i8ixm5cwNFN1TykvIr+R0jny+KzT9ji5u/V3Up2kSNIga32hEh3RQJJkYqmX7fkTy/n8uTf70Fp7t4Up6GeeQprFc/+Fjqt/m1oCjFC9ruWGHZxCWh1hZ1vfq0l5wpMouiylRpXqSnXRA3i7dCply05k36Z7HQSfPvBOu4Q7PMjvp8oPoDPTTP3fONpkFe0BP6WdGd3Yz4ci9kZDtpKegsNOEY+w92UkQbb3myVnyLdqi4c35v9NhS1fF569uBZ+fQpXrdwxkvzr/JTUto2vJOimq+KyM1KqeMicvPkawMDg/cpfqGiGq31ZimtuovWekREbnz3h2RgYPBeYHor6CorJXnHbSIiMryQiRu8rj2WHMgzR1fBBZerUiMhsFdz2qxgq+txC3eUiS19V7R5vo557N7mboRdzQQTbJztx+dlm9FF57+D3dtaB/jHwi9y+m7hM/gujt/6C9KNdMBtOz0M2Rfg7X+HU49Z8qOZjaR7wnZGO3tR4XZ1HX+XpQPYvniunyvjVp9A2vIPv/ktS97zKf4uT5xD6PXMPFJJlwspxzMxuLTN0QGyS9T8hiW3tbLbWuj/iSWHGuHiD3RFyS5bRLpqpoc5+bZXRyw5n3vOkgOZW8gu7MF8HI3yfIQVjj/2NG+JXHMHUm8TEYRszik8c8Hx1yy5Nt9NuquXYQ4CjyG9WVTMY+dtxHXQXODQMaZK4ZBDDAedgUHZw9zsBgZlgml14/OemETbNouIiNvJRf5pmQ25wCu7oRQaM8564GINFNhVKnrQnFKhuVKL3Hhlc4GCV5KZ8wbbarlm99m7Fy5/TmEXzdwRDgVi2+CqzuzmbKO/FlVc2df4+AMucKs5FmJVuZB4jOzyERsn2t4pJCC1OKU9GqvW/U2VZPfCbLjg81NchXdbe4clzyog63DVILuVSwoY75deDJNuewXO4dPt4Fjb4OEmkNPO/7DkI5dxU0/VqzhnSwJ4X42LKw9z4wjZVnJRpXxvEeZKpxEKNOZ4uyqfC5TTmVAz6fJnbbvJnuJdfzNnEK74UrvxWWHe2bfQgCanmVM87aYW/EE3IpxIPMcr6+O/jPtg9A5uXopvKJ33vPO9WY03MDD4LwRzsxsYlAnMzW5gUCaYXsLJs2kp/q9SLO1Ysp11rYhpCnOYF7y2AjqXFzzmiSIfw6FR8eaZwnEuGrGbw9bPUxTeginvB5GAvuUjpKsIIg6NfRPBYbCHq+QCPSctuVN5SKcHkR5MPpIl3d6zGyw59WDEkkMzmNDSNQ/xWqiXK8EcO6MYRweq2LKhVrLz2WLgo/5u0i2rw1pCbRI5tYCLK9yS/4wxOnJMOHLlTHy3G1ZhjSSxhNNaT1ah+3FDluPhbWux3rHhp5jH+mpOO308jcs4MYWAMxlG9aUrgvO+MMvfxT2w2ZK91/PaweAKrCtUbJmyV8ERrBfkPolzkbqOSSQSbqx3+DYzkaTraXQdOlddZslpPxelHrNVdBY+yt2UumYyBdjDHXV2mCe7gUGZwNzsBgZlgul140dd4vhByeXN+jjtVJwF1ya/+N9I1/qb2MX00Gy4SllhV8yXh5vjcHPqTWlUkOU1Gmg8wqmK/DB0xR37Sed+BOQYFX2onKqPcSjQWYTd4myGdCqHysG4k6uxDj2PppzRU9jR1PmXvA1VcRb42IoL2Y0PH0EKM9QN19qteSshtRJucYiL66RBoYLuVBJzn3qWq9NOaKQ+T2W4aShyEuHKsjHwwi26innNP2jjU7/7NPP0j1wDwoft4/9iyRufYm742jiIKALDHFJVJBEKZNchdEk+N4W7XaFRRR/pYN1dcK3dM7jBRR6Fe158Cc0pvutXkFl1CuHE8oWcH8x/FycgE0NlX/8tTBZy8oWtlhzYxNubTcwqpXv1+TNv5sluYFAuMDe7gUGZwNzsBgZlgmmN2bVoyby5x1gvE+YVhpDS8B1nQgmXLS0y8pUH8Z42DjY9eXRNFYWP7xasCbjySE9kz3EqRX8Xdu6THLM7h5FmqZoAqaQvzeSZnSnE8JV5HmM+j7LSY4o5zgdtJb35PqxpuA8zIcNEVTdeLOHf65paxOJjD+MY1f0c29etQ2poxjLWbX4DcfS/70Oq7CM5jkN74jhGIs1bNnfHbVsl25Y0njy5j+xmtmNr46W+GaRbMo5x3Xn/5y35tsKzZJf/F6wxDHu5HFd3Y46zN2LuN1VcTXZNY9g3cFY7Ez2qcZBH5i5jIsmGOsTwvi5cc7m/57WaOR3Y6rn9Zi73zdjWEkZ++Jgln5rD85FYgjFmX7ibdJ6B0iSr+ANyPpgnu4FBmcDc7AYGZYLpJa9wuSReV6oGqxnhrh2dgTtdP9BJOs9zcM9jA0j3OP+Iu7V8a7HNsSvHFUaOXhw//zQ+O9HHXWPOKFx8t3CXV8FGkjASQJVfleLqsfoc8h+OcXbxc4JxbJnSVee3vR5LIq1YWM+/yYUbfg3vEXZpz86AC+pbhznoT09JNQ0jTeQd43zNtvng0Ov1Y662P8HswOoswotQlo+hHOjkyk/gffFty8hu5whIQHZ1cOpt7uDPLHlWF7r21tzInYqN+/FdDm9jAoz2cZyb21/Cd4ktOE5222NIdb46xqlU3y22NOVL7FrH0yA0qTmFTs7KFIeiczp/bMnF2FOk674F57p/M66Bnsc5nMg/gJCwsJL5Fz1bSilXR4LvKzvMk93AoExgbnYDgzLBtLrxAZWTlc5StdOWOUzqEOhBBdbCFLsi3jhsC1tAuuD+yCqyc3wILrK+vpt0nuNwY9NRm/vs5FV7lxOuUnyYidWSBayihoIgJ9BOpgaeoeDCZnJ7STeeAKFEX66DdCkbrbX/FhvH3UfZBQ8uhstclA+QrnAGq88pWzFZyyi72QMac9opPN9dp563ZHXU1qQR48Ydn8f2Pd1TWCO8CMVGfViJLj7JdskYPiv4R8xPd3wuVvu7T8LVbU1wI0n1HyNLkPxH5h68rws03FfNQFhW08GudGICIVr/OFfh/fxVhAYbevl8vjYCXrsKG+9e5+j/ILt59fjsqiyHmIdqcS78NagGdKf4WVw9iHEkIjxXnpbSuXGeZvfeDvNkNzAoE5ib3cCgTGBudgODMsG0xuwNyi2/5y3FxOv83LE2sAzx9uK9XKWUyKHyKViwbROc5tSEfglVbZLi9Eyio9uSHW22mCnKlV960LZlsyNCOlWBLXPTFUj3VA1wRZTP02HJ+QmO+/ensC3VaDVXWVXdtNmST38FHV/uau6OcymsVYTSnDpsG8O4ih7E2IkZTBoRHLoe8iGuFBw5gXEFjmOO/cNMyDA2iu/mb2biidGWKI5/NgK7k0zEESsi/s59jdcEJn6E2LlxHtYw2l6ast1yJ9ZxKr/4AukGH+225O9rbCs9e3Ax2S0YRyXczAFmhPxjH+b0D/0834evw2c/3oHr4JnQZrL7yutI1Vamj5BOrcH7lnwM8xH5Ic9VqCNiybk25vB39U12Wrq4q5BszquxD0apbhGJiUhBRPJa61VKqWoR+bGIdIhIt4h8WGs9dr5jGBgYXFr8Im78DVrr5VrrNx8rXxKRjVrrOSKycfK1gYHB+xTvxI2/S0TWTsoPS2kPuC++3Rsy4ZD0XL9GREQue4Grx8ILULnmdHE6bPtREFGocVTQZfPsBk+MIG3mG5vitl6DPZ8q8zau71puiMgU1ltyxLGZdMNNOIZ7FK7X8imMAa4RpJNixadJt0fDJU/E/ph0zifhggZs3zl/P7ucngjmoDZ2mnTpDMboXIyqM79nK9k17pyLMfWz+5wax/tm98Nd1MMdZJetw/cuNHE1Y+VI1JK9A+ANHCtey3YupKQybbwPQHoA/O1tC23EJHfzzrXxMaQmR3KLSFeYgevszDfRWLM1GyG7iB+VcZdHOMRcMgPXy5JWvmUW1WCn8pVH4XZ/5hBX8h0cRZj6TD3z0r/YiXO4uRkpXf+U67vzDOa7YRG7+A0fLIU2agt/rh0X+2TXIvK8UmqXUurNtrMGrfWAiMjk//XnfbeBgcElx8U+2VdrrfuVUvUiskEpdeSC75jE5I/DgyIiNeHmC1gbGBi8V7ioJ7vWun/y/yEReVRErhCRQaVUk4jI5P9D53nvQ1rrVVrrVaFA1VuZGBgYTAMu+GRXSgVFxKG1jk3Kt4jIX4jIEyLycRH56uT/j5//KCWMB8fl6StKMWxtmnnMr/AjNbG0jdM41WHEZB3diJ/m9HLJbSqNWP9wjKOK0baDGIcLJaCVcU5hBJ1YO4g6+LfQ1YJ4an4rYu+FE9wNFt2BmPII08bLNmXrqksfJN1oCt8nuBMplMxxjmVTHfg8Z00b6ZzOqCU3dOPvEQ+TaCy0bQf843QT6QqD2LPMNYLyy7Eajger6zAfhQTzlVecQ0pzJIdJqAhxmjKlftmSi/t/Srrs8/ietfNwPEc1PzSiT2Nce3/Kz5xTQ1h/iIxjvpt8vHbgqEYZ76Fq7nrrt60rvLaN97SbncXczYkjGdXq53h7rRcdfVcmPk26L2wHmeauz6Ebb/39TG7y6kmsWxw+xdd+Q2vpWo29zV5vF+PGN4jIo0qpN+3/Q2u9Xim1Q0R+opT6lIicFpH7LuJYBgYGlwgXvNm11l0isuwt/j4iIje+F4MyMDB49zGtFXQ5t1sGmkvuZPpqrkS6/TB4xJyVvN1tdAlSIfPb0O1zbhe7lXcchUtbP8rVdY91wcUaWYkOp8PVnLrqb0ZlmSu/h3Q1frjWSmFMJ+r4s573RPFZ3tmkC7+E8WcL7ManulEZN1a0zUeUT5M+DJc59wB3kVUtQSgzZqtOG+eiM7nBtp3zWP9vkC5Qi9Bm/sodlnwuxaGRI45uuZo0c7+N5uzbRiFNlCrwXPnc6FjzXMWkJQ0fed2SL7ddqqH1N5Pd2R++asnJvrmkcxQQivVUw6WfUTGf7KSALrXcyVdYlcYcpD0nSZcLIaXW7UFqr9k9hdswhM8LhgOkC4e+bMktPwTZyQfWvUx2Y6cQUj1fzWHwi8Ol1OdInkNgO0xtvIFBmcDc7AYGZQJzsxsYlAmmNWZ3OZVUh0spq+YGTsE09dti54oO0lVqpCP6gyjtPHKWh39sFL9dq7NcAnrHyyibnH8l4tqRIP/eHa5EWm5rnvnan69AOevTPqwrvOrk+Czpu8GSZ0XfIF1kC2L20BJec8hGH7BkPYKUjku+SnY6Y2PJeZXTZvkbkHpy2dYYmqo5TizsR5nwySznB9VVd1jy+DC6B/1Du8kua9ua+kSCCRYDIVt6LN5hiZkiF1aNP2hjyfnsPaRb1IN9zyKPINYvHufz7mmx7bHmmTIfEyiDdXtxjeUSnClWtg5Bbx13XUZj2HOtmGeWmawgxm6st7HM1HJ6cG/gB5bcfpCZgQ6+jutgaDb21pu5hDs3Z8/FuD7m4O62e1Olcujbi+e/pc2T3cCgTGBudgODMsG0uvFupaTZW6rwWRtsJF2oCmmiZANXKTl9ti1tbZ1Azjy7puk+VBVtzTHhw+kiUmWNDXDFrq1nd+saH9Jaa5zcEXe/D8ff7gKRw4EUV48d6MS2zF0Bdlv1KbiVDRkuX6ich+8d2IFQQOU4facF6RXvIe4QHH8EVVfu27BN86zqy8lu+3ykw7Itj5Ku3gldwgMCx2KBt3/KO5CmzOW5cisZQfrOKSCGcAc5dRXtA3FG4Z/5XFR6kGpyKKTlxuoWkt2gRhXegQHeztk3sd2Sw16kez31TNjos5FA6v49pHOlMP8+7xLSHfFhq+c1zm9Y8u6xbrL7xgQ6Fz/q4eq65GVw43uaEPYdcT1Ddp61CBdXZJjktLOhlC7M+7ma0w7zZDcwKBOYm93AoEwwrW68Q0T8k0QPnQ5eAfY22vjaj7E7N3Y1Xse64S6GK9gdihfhihXnckXX0CGEDalNsBv9EJNcPFfXbclLpuwEu1xhjJ9VNvfczWxcpwUruyeL/F1euwfj3/H0RtIdbt9jyeFzCEP8fUzI0JTEd1H6GtLlHrGRV9RiHLM/8QjZPduIyrvixCnSVfjReDSsui25ah5Xv/nzCEPyfctJl8jZ3PURNJI0NnOjiqcR7n4tnzK5WTBGbxqh0kDVdrJ7w42sSW6UiS3yLTg3kQDkRJxDr4aCjTyl5ijpsl5bOOFivr7GFMKjtEJW4OEEVza+aHs9oTmEXRfDvZDQuzB2xU1OhVaEMrsKvK3Y7pFSBmXMzWQpdpgnu4FBmcDc7AYGZQJzsxsYlAmmNWZ3Kp9UekrxZ8TNvzM6iIBt6PhrpNuxGKmJymbEzYNqSoougvg4EeW0nK8WMVnhqK1a79xmsqs4gUq7w7/EFW6zrthnybkZ6KAKFXgcFSOodJrVyHtvrQgjdv7kvUycse9lrEH8zNa1t9/F6ar9XRhjc46JOxttMXzLE0gZVfwOx6HRGqwDeISrGR02EpBaJ+JJRx3H2+NuzIe7gfdfC+bQIZcbAdll3s3rD34XztmiQj/rsjhmwo21jrN5ntNeX7cl6xauwsvVo+rvjB/zXT2lg687h3MdKnLlZGsqasm+BBOJVGt8dvc5UDrsTvG5dbQiJda4kMefX4pOPf8inPdEO1/DSuGY/ZrXk8JSOkbBxaQWNIbzagwMDP5bwdzsBgZlgml14z3KKW2uUmrEFeD0SToDF9bVWiBdahRuT2YuZKeN31xERK9Dg0FogJtTvB9C5VpVAKkg90zO9+hvwF30/TlX4Xk7MF2f+T7ed4UrSnbBzahiO9DETRUzfTjGZcs5/bjiarhtS5M27jfhKryeOCronpuoI93JLCqwLh9B6JL86yvIzvV3GHOgnsOQpEYqLqsxj8UEn5fCcbi+VUFO37l7MeZiBnOajPB3LrrxPVf4mbzC4YHbOuFEujSh+byMJXHe1QCnB/OR6yzZVXzWkp1ZTtsOuXH8tIPdbJetGjOVZNf6xiJSavsVqvfO3sBpM++taOZa1MnNNAEX5m4sjbCsWOBr0+eypQ5lJulCwVITjsPxznnjDQwM/ovD3OwGBmUCc7MbGJQJpjVm96q8zJmMw7xpjreHtS3Onc1xR89GxKHZ+UjdOINMFhlaiTLEsxuY0NLViFRLphHxWrGCY7ABP+LLliQTYBw4gXTHzjHsbeasOU52s+aA4/z0cxx3da9BTLZnkAkIamsQey6bh3RYw0tc0rtgDbrxrjrAKbX4MNJVrjHE2LGfcYfd4vt+YsldH+RzkbY1sPkFRBmVitM6/gBi6mxxSqpzNi4tf+Gjlpz07CQ7l8O2RXGG91gr2khBYnnEq306TnbxJSBfdFXyGIOp5ZZcNQGCz4KbY/Y2J16Pj/L1lxrF2oevk7f4nnkVztPrjbjm8oFPkt2CEMpg691MAjLs2GPJaSfmY0GS4/6jYaToZipe3/A6SmlWl+K1KjvMk93AoExgbnYDgzLBNKfe8tLiKVUgZV1Tq4Ow5c5EE7u+7nlw3YMxpM3iFZzCULaUUdN9M0iXdYAfLOUDz1yqeIbsmsfhsi0a4e6kH7sRCqRsvm68yL+ZBzvgnmvNKRLfeqTK8r++l3RDDnz2piyIIdIuHmO4gJCnbQqX3xon3NiFPoQ5Q9E1ZBf8HlJjsxYzP3mfH51c4zae9JiP3edAO1JU1WkOSZx5dJ8VNdJfAUcH2c1PgfM94Wdy+z5B+HKgALf1DRdvV5ySBZZcW8McdMEBbLtU6QK3XuYcd8dJFucisJI54nJ9CC9aXuF5zCVw7nfcYdtiy8tdaYuGMcazweWkUzZSjVzgRUs+4OQxOorgwtOOY6Rr1v9TRETcwlWI9P7zauyDUSqilPqZUuqIUuqwUupqpVS1UmqDUur45P9m10YDg/cxLtaN/7qIrNdaz5fSVlCHReRLIrJRaz1HRDZOvjYwMHif4mJ2ca0QketE5AEREa11VkSySqm7RGTtpNnDIrJZRL74dsfSjrwUvaUV3FNZ3qZnzANX6ZifV0Oda3ssOZWHC1tsZGciH4ML6/Cxi6xd4C0rCHbwzGtevQxfjq1/PM+yOzfWg89WBzFeZzPvthnuxwp5ftFq0uVeAqdb7gQ3VfjmYEU468bKf0ueec9O12LFdtDP7nNVBvM6Nwnes6raKNktex7b9N3sWEW60O+Bn21PI87Fi35udtmjsA3TaS+fM+VH6NFUQHil8jeQ3TLHJyx5QDj78ZStQcdXxLntLfIuq3W2lfq4+2ek81aBK9BVgxAqM8DVgCObEGpUj/EYiylkK4LPvUq66PMIbfb973+2ZOc1HGrU1djOU4AzKMnGdZasl6KRKbqsnezqqrDLba6fm6OGx0qhbj7xzrZ/miki50Tk35RSbyil/nVy6+YGrfWAiMjk//VvdxADA4NLi4u52V0islJEvqm1XiEiCfkFXHal1INKqZ1KqZ1jwxMXfoOBgcF7gou52XtFpFdr/Wbnys+kdPMPKqWaREQm/x96qzdrrR/SWq/SWq+qqq14KxMDA4NpwMXsz35WKXVGKTVPa31USnuyH5r893ER+erk/4+/zWFERCSrctLtKsXsT9UuJZ3nHFIT3YUtrHMjNs/btvBxeDn9ULMP6TxvimOmUY00jm5H55Wv/TKyczUjzu3vYA7ykePgYXf8LY6hYpxe0zbubscsXleIjiCG9D3B6cHgvdhCKVCN9YJYNceyFf2IN0eu4nm83otY2ef6IcbIxWnS5UUl38693InWOd5hyTPbfsWS/+cQ2xXUS5a8y8fbXD0ZhO1OF9JB3UHudvy6wtzlNM9HrIi1imbBmByFh8huPIv5bjs3JR0bR3pwpIB5zAZ5Qqp6nrTkisSLpEtV34vjX/YE6Xa8hhRmJoVUWcvLfG0WZiMlOOLgtHPRgTkZ2L4Hn9v2J2SXrUakHD7KXvLEROlWLvTzGo4dF5tn/5yI/EAp5RGRLhH5hJS8gp8opT4lIqdF5L63eb+BgcElxkXd7FrrPSKy6i1UN77F3wwMDN6HmNYKukHxytecpUqosxUcvy+w87il7yWd6wwaLgr98/D3HKeuEoNRS4462I3PBVDxForAZXMKbwPkqYTb3RPi7Y5SYewqWr0X489+nivcEgG4tMmlm0nnnwOyg9bGxaTL93dbcnEEblpwBacph7+P1NBM/2HS1S1FSqZoc6XzXg4FDo9juebU80zWsPebSC95/gA8c4vqP05219fgmEv87D6vcSEMGXXCVX/WyaHRVo35OCxcQZdxY157bJWI1VNSha0JpEujivngQwHMcTSFZaXaWr70czNwPh1NrPMncZ21z72TdN86jC2adBTXWL2P3enovQg1ztZxY5Onr9uS4ymkBwvyb2QX673Lkic0p++qXaUUoENxStEOUxtvYFAmMDe7gUGZwNzsBgZlgmmN2ePaIa/kSrGdTu4i3bkziKcaRhpIp0ZQKqnTSM94crylcqoWKY1g5VOk8/pRRugLRSzZGeWS2NYcSla3B7/FX2AJ0oP+fpQ4Vo/zNrkDi1Ee6hriwsLEXnzPc9WcnokPInaunQmSw6yLY9mJasTptz/bQzpPDWLgXANiyMMVvIbRW48SX9dxjtkdh5FCim3A82D373N33JBt3cUhzJPeLpjHNgfWH357ClHibzrwPU9pLtV40VYuu0lhLeK4436yG3Rh/FUePkaoiDWCNge6Haun9G1FZyEVGV3EBBWrN+O7ZAaZ4HOPLbXq2QWyy84hXo8ZspVGu5bw+slADdZkcjGUb1cNcNxfjOI+cE7MJ12Xq5S+y4jhjTcwKHuYm93AoEygtD7/Y/9d/zClzolIj4jUikzx+y4NzDgYZhyM98M4ftExtGut695KMa03u/WhSu3UWr9VkY4ZhxmHGcd7NAbjxhsYlAnMzW5gUCa4VDf7Qxc2mRaYcTDMOBjvh3G8a2O4JDG7gYHB9MO48QYGZYJpvdmVUrcqpY4qpU4opaaNjVYp9R2l1JBS6oDtb9NOha2UmqGU2jRJx31QKfX5SzEWpZRPKfW6Umrv5Dj+/FKMwzYe5yS/4VOXahxKqW6l1H6l1B6l1M5LOI73jLZ92m52pZRTRL4hIh8UkYUicr9SauHbv+tdw3dF5NYpf7sUVNh5EfkDrfUCEblKRD4zOQfTPZaMiKzTWi8TkeUicqtS6qpLMI438Xkp0ZO/iUs1jhu01sttqa5LMY73jrZdaz0t/0TkahF5zvb6yyLy5Wn8/A4ROWB7fVREmiblJhE5Ol1jsY3hcRG5+VKORUQCIrJbRK68FOMQkdbJC3idiDx1qc6NiHSLSO2Uv03rOESkQkROyeRa2rs9jul041tExM7y0Dv5t0uFS0qFrZTqEJEVIrL9Uoxl0nXeIyWi0A26RCh6Kebkn0Tkj0TEzjpxKcahReR5pdQupdSDl2gc7ylt+3Te7Oot/laWqQClVEhEfi4iX9BaXxJ+ba11QWu9XEpP1iuUUosv8JZ3HUqpO0RkSGu964LG7z1Wa61XSinM/IxS6roLveE9wDuibb8QpvNm7xURO31oq8jb7EL33uOiqLDfbSil3FK60X+gtX7kUo5FRERrHZXSbj63XoJxrBaRO5VS3SLyIxFZp5T6/iUYh2it+yf/HxKRR0XkikswjndE234hTOfNvkNE5iilOidZan9FRJ64wHveSzwhJQpskYukwn6nUEopEfm2iBzWWv/jpRqLUqpOKRWZlP0icpOIHJnucWitv6y1btVad0jpenhRa/2x6R6HUiqolAq/KYvILSJyYLrHobU+KyJnlFJvEi2+Sdv+7ozjvV74mLLQcJuIHBORkyLy/07j5/5QRAZEJCelX89PiUiNlBaGjk/+Xz0N41gjpdBln4jsmfx323SPRUSWisgbk+M4ICJ/Mvn3aZ8T25jWChbopns+ZorI3sl/B9+8Ni/RNbJcRHZOnpvHRKTq3RqHqaAzMCgTmAo6A4MygbnZDQzKBOZmNzAoE5ib3cCgTGBudgODMoG52Q0MygTmZjcwKBOYm93AoEzw/wG3uzBNYgyD4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.uint8(x_train[2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c48bfac4-2913-4526-a7c6-1fbcc35ca1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7890d-806e-4dd1-be3c-4981fd115dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614c046-ec5b-470d-bdc0-3da0bd262f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be76ef-6ca4-4bdd-aeaf-4af90e144a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f06bbc71-96dd-4d32-9186-535422a85428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder architecture\n",
    "def get_encoder():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            #keras.Input(shape=(image_size, image_size, image_channels)),\n",
    "            layers.Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(64,64,3)),\n",
    "            #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
    "            #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
    "            #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
    "            #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            #layers.Dense(width, activation=\"relu\"),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            \n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "237045a1-a8cf-4aa1-8715-b39cff9a2203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04f1dabc-38a9-454e-9af6-937351d2b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Baseline supervised training with random initialization\n",
    "baseline_model = keras.Sequential(\n",
    "    [\n",
    "        #keras.Input(shape=(image_size, image_size, image_channels)),\n",
    "        #get_augmenter(**classification_augmentation),\n",
    "        get_encoder(),\n",
    "        layers.Dense(154), #layers.Dense(10),\n",
    "        \n",
    "    ],\n",
    "    name=\"baseline_model\",\n",
    ")\n",
    "baseline_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    # sparse_categorical_crossentropy\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bcc9dac-ba0e-49fe-a61d-7611b758e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 7/10 [====================>.........] - ETA: 16s - loss: 177.5270 - acc: 0.0050"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-6c634bfe2eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m baseline_history = baseline_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#labeled_train_dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_history = baseline_model.fit(\n",
    "    x_train, y_train, #labeled_train_dataset, \n",
    "    epochs=num_epochs, \n",
    "    steps_per_epoch=10,\n",
    "    batch_size = 256,\n",
    "    #validation_data=test_dataset\n",
    ")\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(baseline_history.history[\"val_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866bc9a-1bdc-4698-91c2-70f6d42786ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1f48182-dde0-4734-8615-faacc3683b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the contrastive model with model-subclassing\n",
    "class ContrastiveModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n",
    "        self.classification_augmenter = get_augmenter(**classification_augmentation)\n",
    "        self.encoder = get_encoder()\n",
    "        # Non-linear MLP as projection head\n",
    "        self.projection_head = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(width,)),\n",
    "                layers.Dense(width, activation=\"relu\"),\n",
    "                layers.Dense(width),\n",
    "            ],\n",
    "            name=\"projection_head\",\n",
    "        )\n",
    "        # Single dense layer for linear probing\n",
    "        self.linear_probe = keras.Sequential(\n",
    "            [layers.Input(shape=(width,)), layers.Dense(10)], name=\"linear_probe\"\n",
    "        )\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.projection_head.summary()\n",
    "        self.linear_probe.summary()\n",
    "\n",
    "    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.contrastive_optimizer = contrastive_optimizer\n",
    "        self.probe_optimizer = probe_optimizer\n",
    "\n",
    "        # self.contrastive_loss will be defined as a method\n",
    "        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n",
    "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"c_acc\"\n",
    "        )\n",
    "        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n",
    "        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.contrastive_loss_tracker,\n",
    "            self.contrastive_accuracy,\n",
    "            self.probe_loss_tracker,\n",
    "            self.probe_accuracy,\n",
    "        ]\n",
    "\n",
    "    def contrastive_loss(self, projections_1, projections_2):\n",
    "        # InfoNCE loss (information noise-contrastive estimation)\n",
    "        # NT-Xent loss (normalized temperature-scaled cross entropy)\n",
    "\n",
    "        # Cosine similarity: the dot product of the l2-normalized feature vectors\n",
    "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
    "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
    "        similarities = (\n",
    "            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n",
    "        )\n",
    "\n",
    "        # The similarity between the representations of two augmented views of the\n",
    "        # same image should be higher than their similarity with other views\n",
    "        batch_size = tf.shape(projections_1)[0]\n",
    "        contrastive_labels = tf.range(batch_size)\n",
    "        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n",
    "        self.contrastive_accuracy.update_state(\n",
    "            contrastive_labels, tf.transpose(similarities)\n",
    "        )\n",
    "\n",
    "        # The temperature-scaled similarities are used as logits for cross-entropy\n",
    "        # a symmetrized version of the loss is used here\n",
    "        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n",
    "            contrastive_labels, similarities, from_logits=True\n",
    "        )\n",
    "        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n",
    "            contrastive_labels, tf.transpose(similarities), from_logits=True\n",
    "        )\n",
    "        return (loss_1_2 + loss_2_1) / 2\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (unlabeled_images, _), (labeled_images, labels) = data\n",
    "\n",
    "        print(\"len(data)=\", len(data))\n",
    "        print(\"len(unlabeled_images)=\", len(unlabeled_images))\n",
    "        print(\"len(labeled_images)=\", len(labeled_images))\n",
    "        print(\"data.shape=\",data.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Both labeled and unlabeled images are used, without labels\n",
    "        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n",
    "        # Each image is augmented twice, differently\n",
    "        augmented_images_1 = self.contrastive_augmenter(images)\n",
    "        augmented_images_2 = self.contrastive_augmenter(images)\n",
    "        with tf.GradientTape() as tape:\n",
    "            features_1 = self.encoder(augmented_images_1)\n",
    "            features_2 = self.encoder(augmented_images_2)\n",
    "            # The representations are passed through a projection mlp\n",
    "            projections_1 = self.projection_head(features_1)\n",
    "            projections_2 = self.projection_head(features_2)\n",
    "            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
    "        gradients = tape.gradient(\n",
    "            contrastive_loss,\n",
    "            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "        )\n",
    "        self.contrastive_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients,\n",
    "                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "            )\n",
    "        )\n",
    "        self.contrastive_loss_tracker.update_state(contrastive_loss)\n",
    "\n",
    "        # Labels are only used in evalutation for an on-the-fly logistic regression\n",
    "        preprocessed_images = self.classification_augmenter(labeled_images)\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(preprocessed_images)\n",
    "            class_logits = self.linear_probe(features)\n",
    "            probe_loss = self.probe_loss(labels, class_logits)\n",
    "        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
    "        self.probe_optimizer.apply_gradients(\n",
    "            zip(gradients, self.linear_probe.trainable_weights)\n",
    "        )\n",
    "        self.probe_loss_tracker.update_state(probe_loss)\n",
    "        self.probe_accuracy.update_state(labels, class_logits)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        labeled_images, labels = data\n",
    "\n",
    "        # For testing the components are used with a training=False flag\n",
    "        preprocessed_images = self.classification_augmenter(\n",
    "            labeled_images, training=False\n",
    "        )\n",
    "        features = self.encoder(preprocessed_images, training=False)\n",
    "        class_logits = self.linear_probe(features, training=False)\n",
    "        probe_loss = self.probe_loss(labels, class_logits)\n",
    "        self.probe_loss_tracker.update_state(probe_loss)\n",
    "        self.probe_accuracy.update_state(labels, class_logits)\n",
    "\n",
    "        # Only the probe metrics are logged at test time\n",
    "        return {m.name: m.result() for m in self.metrics[2:]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cf94f5d-19f5-48f5-b011-f718b13ed673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 62, 62, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 60, 60, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 58, 58, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               102760960 \n",
      "=================================================================\n",
      "Total params: 102,873,536\n",
      "Trainable params: 102,873,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"projection_head\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"linear_probe\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,290\n",
      "Trainable params: 1,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Contrastive pretraining\n",
    "pretraining_model = ContrastiveModel()\n",
    "pretraining_model.compile(\n",
    "    contrastive_optimizer=keras.optimizers.Adam(),\n",
    "    probe_optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e61723ed-ad5d-4c83-b0f5-8c153c795a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "len(data)= 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    <ipython-input-22-11e193cd7d7d>:87 train_step\n        print(\"len(unlabeled_images)=\", len(unlabeled_images))\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:873 __len__\n        raise TypeError(\"len is not well defined for symbolic Tensors. ({}) \"\n\n    TypeError: len is not well defined for symbolic Tensors. (IteratorGetNext:0) Please call `x.shape` rather than `len(x)` for shape information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b926cce4d8f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pretraining_history = pretraining_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m print(\n\u001b[1;32m      5\u001b[0m     \"Maximal validation accuracy: {:.2f}%\".format(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    <ipython-input-22-11e193cd7d7d>:87 train_step\n        print(\"len(unlabeled_images)=\", len(unlabeled_images))\n    /usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:873 __len__\n        raise TypeError(\"len is not well defined for symbolic Tensors. ({}) \"\n\n    TypeError: len is not well defined for symbolic Tensors. (IteratorGetNext:0) Please call `x.shape` rather than `len(x)` for shape information.\n"
     ]
    }
   ],
   "source": [
    "pretraining_history = pretraining_model.fit(\n",
    "    train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
    ")\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(pretraining_history.history[\"val_p_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115bcbe-3edf-4762-854d-02acb8c27a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised finetuning of the pretrained encoder\n",
    "finetuning_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(image_size, image_size, image_channels)),\n",
    "        get_augmenter(**classification_augmentation),\n",
    "        pretraining_model.encoder,\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"finetuning_model\",\n",
    ")\n",
    "finetuning_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "finetuning_history = finetuning_model.fit(\n",
    "    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
    ")\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(finetuning_history.history[\"val_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008019f8-eb5e-4b05-a1a9-a6ef666c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification accuracies of the baseline and the pretraining + finetuning process:\n",
    "def plot_training_curves(pretraining_history, finetuning_history, baseline_history):\n",
    "    for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):\n",
    "        plt.figure(figsize=(8, 5), dpi=100)\n",
    "        plt.plot(\n",
    "            baseline_history.history[f\"val_{metric_key}\"], label=\"supervised baseline\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            pretraining_history.history[f\"val_p_{metric_key}\"],\n",
    "            label=\"self-supervised pretraining\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            finetuning_history.history[f\"val_{metric_key}\"],\n",
    "            label=\"supervised finetuning\",\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.title(f\"Classification {metric_name} during training\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(f\"validation {metric_name}\")\n",
    "\n",
    "\n",
    "plot_training_curves(pretraining_history, finetuning_history, baseline_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
